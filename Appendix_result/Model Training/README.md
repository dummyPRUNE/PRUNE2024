# Model Training
The activation function of all FC models we use is ReLU for the hidden layer and softmax for the output layer. We set the maximum number of training iterations to be 150. Cross-entropy is used as the loss function during training. The optimizer setting is SGD with momentum of 0.5. For the MNIST and Fashion-MNIST datasets, the learning rate is set to 0.1. The other two datasets were set to 0.01. VGG16 model architecture used in our experiment is consistent with the official version. The maximum number of training epochs is set to 100. The learning rate is set to 0.01. The settings for the loss function and optimizer remain the same as described above. The operating environment of our experiment is Intel Core i9-12900H with 32 GB of memory and a single NVIDIA GeForce RTX 3070 Ti. 

